Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the data provided, explain the need to standardize the attribute values.
R1: The attributes have different ranges and scales. Hence, there is the need to standardize in order to deal with every feature uniformly.


Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: The parameters are calculated using the examples in the training set. For each column (feature), we compute the mean and the stdev of that column over all the examples. The same parameters are then used to standardize the test set.


Q3: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: We separated the training data (X, after shuffling) into 2 matrices: X0, those who have class = 0, and X1, the others. The probabilities are then obtained from the number of lines of each matrix:   probs = (len(X0)/len(X), len(X1/len(X)))


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: First of all, the classifier creates 4*2 = 8 Kernel Density objects, one for each combination (attribute_j, class_k), and uses each of them to estimate the corresponding distribution p(xj|C=k).
Then, for an example x, it computes, for each class k, a value proportional to the joint distribution p(x, C=k): ln p(C=k) + sum(j=1:N){ ln p(xj=xj|C=k) } , where each p(xj=xj|C=k) is given by the corresponding kde.
Finally, the predicted class corresponds to the one which maximizes this value. 


Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: The bandwidth is a parameter that smooths the fit of the predicted probability densities to the data points. While a bandwidth too large smooths the kernel curve too much, leading to underfitting, a bandwidth too small tends to adjust to ungeneralizable details of the data, and thus, overfit.


Q6: Explain what effect the gamma parameter has on the SVM classifier.
R6: Gamma is a parameter to the Gaussian RBF, and basically defines how far the influence of a single training example reaches. If the gamma is too high, only closer points have influence, leading to overfitting. If the gamma is too low, the points further away carry more weight, leading to straighter decision boundaries and, in general, underfitting.


Q7: Explain how you determined the best bandwidth and gamma parameters for your classifier and the SVM classifier. You may include a relevant piece of your code if this helps you explain.
R7: We estimated, via cross validation, the true error associated to each model (each value of bandwidth/gamma) and then selected the model (the value of bandwidth/gamma) that minimized that estimate, prioritizing simpler models.


Q8: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R8: By calling the classifier's .fit() method (fitting to the training set).


Q9: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the two provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R9: For Naive Bayes with Kernel Density Estimation, the best parameter for the bandwidth was 0.067, the estimate of true error was 0.06, and, the ranges for the approximate normal test are ]84.00 , 101.34[.
As for the Gaussian Naive Bayes, the estimate of the true error was 0.094, while the range for the approximate normal test was  ]97.74 , 138.26[
Finally, for SVM, the best value for the parameter gamma was 0.60, the estimate of the true error was 0.043, and, the approximate normal test range was ]41.66 , 70.33[

The values for the McNemar test between classifiers were the following:
Mcnemar for KDE Naive Bayes & Gaussian NB 18.06
Mcnemar for KDE Naive Bayes & SVM 13.06
Mcnemar for Gaussian NB & SVM 37.68

We can conclude from these stats that every classifier has a different performance, having Support Vector Machiens and KDE Naive Bayes the least difference in performance.
In terms of estimation of the true error, Support Vector Machines stands out of the three, having the lower estimate true error, while, Naive Bayes with Kernel Density Estimation comes after, and finally, Gaussian Naive Bayes.
The best classifier to use in this case would be either SVM, or, Naive Bayes with Kernel Density Estimation.

Q10: (Optional) Show the estimate of the true error of the optimized SVM classifier (if you did the optional part of the work) and discuss whether it was worth doing this optimization. If you did not do the optional part leave this answer blank.
R10:

