Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the data provided, explain the need to standardize the attribute values.
R1: The attributes have different ranges and scales. Hence, there is the need to standardize in order to deal with every feature uniformly.



Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: The parameters are calculated using the examples in the training set. For each column (feature), we compute the mean and the stdev of that column over all the examples. The same parameters are then used to standardize the test set.


Q3: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: We separated the training data (X, after shuffling) into 2 matrices: X0, those who have class = 0, and X1, the others. The probabilities are then obtained from the number of lines of each matrix:   probs = (len(X0)/len(X), len(X1/len(X)))


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: First of all, the classifier creates 4*2 = 8 Kernel Density objects, one for each combination (attribute_j, class_k), and uses each of them to estimate the corresponding distribution p(xj|C=k).
Then, for an example x, it computes for each class a value proportional to the joint distribution p(x, C=k): ln p(C=k) + sum(j=1:N){ ln p(xj=xj|C=k) } , where each p(xj=xj|C=k) is given by the respective kde.
Finally, the predicted class corresponds to the one which maximizes this value. 


Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: A bandwidth too large smooths the kernel curve too much, leading to underfitting. Conversely, a bandwidth too small leads to overfitting. 


Q6: Explain what effect the gamma parameter has on the SVM classifier.
R6:


Q7: Explain how you determined the best bandwidth and gamma parameters for your classifier and the SVM classifier. You may include a relevant piece of your code if this helps you explain.
R7: We estimated, via cross validation, the true error associated to each model (each value of bandwidth/gamma) and then selected the model (the value of bandwidth/gamma) that minimized that estimate, prioritizing simpler models.


Q8: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R8: By calling the classifier's .fit() method (fitting to the training set).


Q9: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the two provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R9:


Q10: (Optional) Show the estimate of the true error of the optimized SVM classifier (if you did the optional part of the work) and discuss whether it was worth doing this optimization. If you did not do the optional part leave this answer blank.
R10:

